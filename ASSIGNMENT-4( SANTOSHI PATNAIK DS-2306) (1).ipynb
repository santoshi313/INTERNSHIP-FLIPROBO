{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6df3b40",
   "metadata": {},
   "source": [
    "1. Scrape the details of most viewed videos on YouTube from Wikipedia. Url = https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos You need to find following details:\n",
    "A) Rank\n",
    "B) Name\n",
    "C) Artist\n",
    "D) Upload date\n",
    "E) Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11cad6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: selenium in c:\\users\\gouri patnaik\\appdata\\roaming\\python\\python39\\site-packages (4.11.2)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\gouri patnaik\\appdata\\roaming\\python\\python39\\site-packages (from selenium) (0.10.3)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (2022.9.14)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\gouri patnaik\\appdata\\roaming\\python\\python39\\site-packages (from selenium) (0.22.2)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (1.26.11)\n",
      "Requirement already satisfied: idna in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.3)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in c:\\users\\gouri patnaik\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.1.2)\n",
      "Requirement already satisfied: outcome in c:\\users\\gouri patnaik\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: sniffio in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Requirement already satisfied: sortedcontainers in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: attrs>=20.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.4.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\gouri patnaik\\appdata\\roaming\\python\\python39\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\gouri patnaik\\appdata\\roaming\\python\\python39\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f2325dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "\n",
    "import selenium\n",
    "import pandas as pd\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#importing selenium webdriver\n",
    "\n",
    "from selenium import webdriver\n",
    "\n",
    "#importing required exceptions which need to be handled\n",
    "from selenium.common.exceptions import StaleElementReferenceException,NoSuchElementException\n",
    "\n",
    "#importing requests\n",
    "import requests\n",
    "from selenium.webdriver.common.by import By\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8718470d",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome()\n",
    "driver.get('https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6148fdf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 30 30 30 30\n"
     ]
    }
   ],
   "source": [
    "RANK=[]\n",
    "NAME=[]\n",
    "ARTIST=[]\n",
    "UPLOAD_DATE=[]\n",
    "VIEWS=[]\n",
    "try:\n",
    "    rank_tag=driver.find_elements(By.XPATH,'//table[@class=\"wikitable sortable jquery-tablesorter\"][1]/tbody/tr/td[1]')\n",
    "    for i in rank_tag:\n",
    "        RANK.append(i.text)            # to scrap rank\n",
    "except:\n",
    "    RANK.append('-')\n",
    "    \n",
    "try:     \n",
    "    name_tag=driver.find_elements(By.XPATH,'//table[@class=\"wikitable sortable jquery-tablesorter\"][1]/tbody/tr/td[2]')  \n",
    "    for i in name_tag:\n",
    "        NAME.append(i.text)             # to scrsp names\n",
    "except NoSuchElementException:\n",
    "    NAME.append('-')\n",
    "\n",
    "try:\n",
    "    artist_tag=driver.find_elements(By.XPATH,'//table[@class=\"wikitable sortable jquery-tablesorter\"][1]/tbody/tr/td[3]')   \n",
    "    for i in artist_tag:\n",
    "        ARTIST.append(i.text)      # to scrap artist details\n",
    "except NoSuchElementException:\n",
    "    ARTIST.append('-')\n",
    "    \n",
    "try:\n",
    "    date_tag=driver.find_elements(By.XPATH,'//table[@class=\"wikitable sortable jquery-tablesorter\"][1]/tbody/tr/td[5]') \n",
    "    for i in date_tag:\n",
    "        UPLOAD_DATE.append(i.text)          # to scrap upload date\n",
    "except NoSuchElementException:\n",
    "    UPLOAD_DATE.append('-')\n",
    "\n",
    "try:\n",
    "    views_tag=driver.find_elements(By.XPATH,'//table[@class=\"wikitable sortable jquery-tablesorter\"][1]/tbody/tr/td[4]')   \n",
    "    for i in views_tag:\n",
    "        VIEWS.append(i.text)        # to scrap views\n",
    "        \n",
    "except NoSuchElementException:\n",
    "    VIEWS.append('-')\n",
    "    \n",
    "print(len(RANK),len(NAME),len(ARTIST),len(UPLOAD_DATE),len(VIEWS))    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "408288e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RANK</th>\n",
       "      <th>NAME</th>\n",
       "      <th>ARTIST</th>\n",
       "      <th>UPLOAD_DATE</th>\n",
       "      <th>VIEWS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.</td>\n",
       "      <td>\"Baby Shark Dance\"[6]</td>\n",
       "      <td>Pinkfong Baby Shark - children's songs</td>\n",
       "      <td>June 17, 2016</td>\n",
       "      <td>13.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.</td>\n",
       "      <td>\"Despacito\"[9]</td>\n",
       "      <td>Luis Fonsi</td>\n",
       "      <td>January 12, 2017</td>\n",
       "      <td>8.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.</td>\n",
       "      <td>\"Johny Johny Yes Papa\"[16]</td>\n",
       "      <td>LooLoo Kids - nursery rhymes</td>\n",
       "      <td>October 8, 2016</td>\n",
       "      <td>6.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.</td>\n",
       "      <td>\"Bath Song\"[17]</td>\n",
       "      <td>Cocomelon - nursery rhymes</td>\n",
       "      <td>May 2, 2018</td>\n",
       "      <td>6.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.</td>\n",
       "      <td>\"Shape of You\"[18]</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>January 30, 2017</td>\n",
       "      <td>6.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.</td>\n",
       "      <td>\"See You Again\"[21]</td>\n",
       "      <td>Wiz Khalifa</td>\n",
       "      <td>April 6, 2015</td>\n",
       "      <td>5.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7.</td>\n",
       "      <td>\"Wheels on the Bus\"[26]</td>\n",
       "      <td>Cocomelon - nursery rhymes</td>\n",
       "      <td>May 24, 2018</td>\n",
       "      <td>5.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.</td>\n",
       "      <td>\"Phonics Song with Two Words\"[27]</td>\n",
       "      <td>ChuChu TV - children's songs</td>\n",
       "      <td>March 6, 2014</td>\n",
       "      <td>5.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9.</td>\n",
       "      <td>\"Uptown Funk\"[28]</td>\n",
       "      <td>Mark Ronson</td>\n",
       "      <td>November 19, 2014</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10.</td>\n",
       "      <td>\"Learning Colors â€“ Colorful Eggs on a Farm\"[29]</td>\n",
       "      <td>Miroshka TV - children's songs</td>\n",
       "      <td>February 27, 2018</td>\n",
       "      <td>4.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11.</td>\n",
       "      <td>\"Gangnam Style\"[30]</td>\n",
       "      <td>Psy</td>\n",
       "      <td>July 15, 2012</td>\n",
       "      <td>4.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12.</td>\n",
       "      <td>\"Masha and the Bear â€“ Recipe for Disaster\"[35]</td>\n",
       "      <td>Get Movies - children's songs</td>\n",
       "      <td>January 31, 2012</td>\n",
       "      <td>4.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13.</td>\n",
       "      <td>\"Dame Tu Cosita\"[36]</td>\n",
       "      <td>El Chombo</td>\n",
       "      <td>April 5, 2018</td>\n",
       "      <td>4.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14.</td>\n",
       "      <td>\"Axel F\"[37]</td>\n",
       "      <td>Crazy Frog</td>\n",
       "      <td>June 16, 2009</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15.</td>\n",
       "      <td>\"Sugar\"[38]</td>\n",
       "      <td>Maroon 5</td>\n",
       "      <td>January 14, 2015</td>\n",
       "      <td>3.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16.</td>\n",
       "      <td>\"Roar\"[39]</td>\n",
       "      <td>Katy Perry</td>\n",
       "      <td>September 5, 2013</td>\n",
       "      <td>3.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17.</td>\n",
       "      <td>\"Counting Stars\"[40]</td>\n",
       "      <td>OneRepublic</td>\n",
       "      <td>May 31, 2013</td>\n",
       "      <td>3.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18.</td>\n",
       "      <td>\"Baa Baa Black Sheep\"[41]</td>\n",
       "      <td>Cocomelon - nursery rhymes</td>\n",
       "      <td>June 25, 2018</td>\n",
       "      <td>3.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19.</td>\n",
       "      <td>\"Sorry\"[42]</td>\n",
       "      <td>Justin Bieber</td>\n",
       "      <td>October 22, 2015</td>\n",
       "      <td>3.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20.</td>\n",
       "      <td>\"Waka Waka (This Time for Africa)\"[43]</td>\n",
       "      <td>Shakira</td>\n",
       "      <td>June 4, 2010</td>\n",
       "      <td>3.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21.</td>\n",
       "      <td>\"Thinking Out Loud\"[44]</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>October 7, 2014</td>\n",
       "      <td>3.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22.</td>\n",
       "      <td>\"Lakdi Ki Kathi\"[45]</td>\n",
       "      <td>Jingle Toons</td>\n",
       "      <td>June 14, 2018</td>\n",
       "      <td>3.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23.</td>\n",
       "      <td>\"Dark Horse\"[46]</td>\n",
       "      <td>Katy Perry</td>\n",
       "      <td>February 20, 2014</td>\n",
       "      <td>3.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24.</td>\n",
       "      <td>\"Perfect\"[47]</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>November 9, 2017</td>\n",
       "      <td>3.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25.</td>\n",
       "      <td>\"Faded\"[48]</td>\n",
       "      <td>Alan Walker</td>\n",
       "      <td>December 3, 2015</td>\n",
       "      <td>3.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26.</td>\n",
       "      <td>\"Let Her Go\"[49]</td>\n",
       "      <td>Passenger</td>\n",
       "      <td>July 25, 2012</td>\n",
       "      <td>3.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27.</td>\n",
       "      <td>\"Humpty the train on a fruits ride\"[50]</td>\n",
       "      <td>Kiddiestv Hindi - children's songs</td>\n",
       "      <td>January 26, 2018</td>\n",
       "      <td>3.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28.</td>\n",
       "      <td>\"Girls Like You\"[51]</td>\n",
       "      <td>Maroon 5</td>\n",
       "      <td>May 31, 2018</td>\n",
       "      <td>3.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29.</td>\n",
       "      <td>\"Bailando\"[52]</td>\n",
       "      <td>Enrique Iglesias</td>\n",
       "      <td>April 11, 2014</td>\n",
       "      <td>3.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30.</td>\n",
       "      <td>\"Lean On\"[53]</td>\n",
       "      <td>Major Lazer</td>\n",
       "      <td>March 22, 2015</td>\n",
       "      <td>3.43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RANK                                             NAME  \\\n",
       "0    1.                            \"Baby Shark Dance\"[6]   \n",
       "1    2.                                   \"Despacito\"[9]   \n",
       "2    3.                       \"Johny Johny Yes Papa\"[16]   \n",
       "3    4.                                  \"Bath Song\"[17]   \n",
       "4    5.                               \"Shape of You\"[18]   \n",
       "5    6.                              \"See You Again\"[21]   \n",
       "6    7.                          \"Wheels on the Bus\"[26]   \n",
       "7    8.                \"Phonics Song with Two Words\"[27]   \n",
       "8    9.                                \"Uptown Funk\"[28]   \n",
       "9   10.  \"Learning Colors â€“ Colorful Eggs on a Farm\"[29]   \n",
       "10  11.                              \"Gangnam Style\"[30]   \n",
       "11  12.   \"Masha and the Bear â€“ Recipe for Disaster\"[35]   \n",
       "12  13.                             \"Dame Tu Cosita\"[36]   \n",
       "13  14.                                     \"Axel F\"[37]   \n",
       "14  15.                                      \"Sugar\"[38]   \n",
       "15  16.                                       \"Roar\"[39]   \n",
       "16  17.                             \"Counting Stars\"[40]   \n",
       "17  18.                        \"Baa Baa Black Sheep\"[41]   \n",
       "18  19.                                      \"Sorry\"[42]   \n",
       "19  20.           \"Waka Waka (This Time for Africa)\"[43]   \n",
       "20  21.                          \"Thinking Out Loud\"[44]   \n",
       "21  22.                             \"Lakdi Ki Kathi\"[45]   \n",
       "22  23.                                 \"Dark Horse\"[46]   \n",
       "23  24.                                    \"Perfect\"[47]   \n",
       "24  25.                                      \"Faded\"[48]   \n",
       "25  26.                                 \"Let Her Go\"[49]   \n",
       "26  27.          \"Humpty the train on a fruits ride\"[50]   \n",
       "27  28.                             \"Girls Like You\"[51]   \n",
       "28  29.                                   \"Bailando\"[52]   \n",
       "29  30.                                    \"Lean On\"[53]   \n",
       "\n",
       "                                    ARTIST        UPLOAD_DATE  VIEWS  \n",
       "0   Pinkfong Baby Shark - children's songs      June 17, 2016  13.18  \n",
       "1                               Luis Fonsi   January 12, 2017   8.23  \n",
       "2             LooLoo Kids - nursery rhymes    October 8, 2016   6.76  \n",
       "3               Cocomelon - nursery rhymes        May 2, 2018   6.33  \n",
       "4                               Ed Sheeran   January 30, 2017   6.05  \n",
       "5                              Wiz Khalifa      April 6, 2015   5.98  \n",
       "6               Cocomelon - nursery rhymes       May 24, 2018   5.46  \n",
       "7             ChuChu TV - children's songs      March 6, 2014   5.42  \n",
       "8                              Mark Ronson  November 19, 2014   5.00  \n",
       "9           Miroshka TV - children's songs  February 27, 2018   4.94  \n",
       "10                                     Psy      July 15, 2012   4.86  \n",
       "11           Get Movies - children's songs   January 31, 2012   4.55  \n",
       "12                               El Chombo      April 5, 2018   4.41  \n",
       "13                              Crazy Frog      June 16, 2009   4.00  \n",
       "14                                Maroon 5   January 14, 2015   3.91  \n",
       "15                              Katy Perry  September 5, 2013   3.84  \n",
       "16                             OneRepublic       May 31, 2013   3.84  \n",
       "17              Cocomelon - nursery rhymes      June 25, 2018   3.73  \n",
       "18                           Justin Bieber   October 22, 2015   3.69  \n",
       "19                                 Shakira       June 4, 2010   3.68  \n",
       "20                              Ed Sheeran    October 7, 2014   3.63  \n",
       "21                            Jingle Toons      June 14, 2018   3.63  \n",
       "22                              Katy Perry  February 20, 2014   3.56  \n",
       "23                              Ed Sheeran   November 9, 2017   3.51  \n",
       "24                             Alan Walker   December 3, 2015   3.49  \n",
       "25                               Passenger      July 25, 2012   3.48  \n",
       "26      Kiddiestv Hindi - children's songs   January 26, 2018   3.51  \n",
       "27                                Maroon 5       May 31, 2018   3.45  \n",
       "28                        Enrique Iglesias     April 11, 2014   3.43  \n",
       "29                             Major Lazer     March 22, 2015   3.43  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1={\"RANK\":RANK,\"NAME\":NAME,\"ARTIST\":ARTIST,\"UPLOAD_DATE\":UPLOAD_DATE,\"VIEWS\":VIEWS}\n",
    "df1=pd.DataFrame(data1)\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7107cf46",
   "metadata": {},
   "source": [
    "2. Scrape the details team Indiaâ€™s international fixtures from bcci.tv.\n",
    "Url = https://www.bcci.tv/.\n",
    "You need to find following details:\n",
    "A) Match title (I.e. 1 ODI)\n",
    "B) Series\n",
    "C) Place\n",
    "D) Date\n",
    "E) Time\n",
    "Note: - From bcci.tv home page you have reach to the international fixture page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "76600b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome()\n",
    "driver.get('https://www.bcci.tv/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "39544b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "button=driver.find_element(By.XPATH,'//button[@class=\"navbar-toggler menu-btn menu-icon\"]')\n",
    "button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3c17bf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "international=driver.find_element(By.XPATH,'//a[@class=\"nav-link \"]')\n",
    "international.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4a7fdc0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 8 8 8 8\n"
     ]
    }
   ],
   "source": [
    "MATCH_TITLE=[]\n",
    "SERIES=[]\n",
    "PLACE=[]\n",
    "DATE=[]\n",
    "TIME=[]\n",
    "try:\n",
    "    title_tag=driver.find_elements(By.XPATH,'//div[@class=\"match-card-bottom\"]/div/span[1]')\n",
    "    for i in title_tag:\n",
    "        MATCH_TITLE.append(i.text)      # to scrap the title\n",
    "except NoSuchElementException:\n",
    "    MATCH_TITLE.append('-')\n",
    "\n",
    "try:\n",
    "    series_tag=driver.find_elements(By.XPATH,'//div[@class=\"match-card-bottom\"][1]/div/span[1]')    \n",
    "    for i in series_tag:\n",
    "        SERIES.append(i.text.split(' ')[0])           #to scrap series details\n",
    "except NoSuchElementException:\n",
    "    SERIES.append('-')\n",
    "\n",
    "try:\n",
    "    place_tag=driver.find_elements(By.XPATH,'//div[@class=\"match-card-bottom\"][1]/div/span[2]')  \n",
    "    for i in place_tag:\n",
    "        PLACE.append(i.text)        # to scrap the place details\n",
    "except NoSuchElementException:\n",
    "    PLACE.append('-')\n",
    "\n",
    "try:\n",
    "    date_tag=driver.find_elements(By.XPATH,'//div[@class=\"match-card-top\"][1]/div/div[1]') \n",
    "    for i in date_tag:\n",
    "        DATE.append(i.text)            # to scrap date\n",
    "except NoSuchElementException:\n",
    "    PLACE.append('-')\n",
    "try:\n",
    "    time_tag=driver.find_elements(By.XPATH,'//div[@class=\"match-card-top\"][1]/div/div[2]')    \n",
    "    for i in time_tag:\n",
    "        TIME.append(i.text)    # to scrap time details                        \n",
    "                              \n",
    "except NoSuchElementException:\n",
    "    TIME.append('-')\n",
    "                             \n",
    "                            \n",
    "print(len(MATCH_TITLE),len(SERIES),len(PLACE),len(DATE),len(TIME))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e6b22139",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MATCH TITLE</th>\n",
       "      <th>SERIES</th>\n",
       "      <th>PLACE</th>\n",
       "      <th>DATE</th>\n",
       "      <th>TIME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1st T20I -</td>\n",
       "      <td>1st</td>\n",
       "      <td>The Village,</td>\n",
       "      <td>18 AUG 2023</td>\n",
       "      <td>7:30 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2nd T20I -</td>\n",
       "      <td>2nd</td>\n",
       "      <td>The Village,</td>\n",
       "      <td>20 AUG 2023</td>\n",
       "      <td>7:30 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3rd T20I -</td>\n",
       "      <td>3rd</td>\n",
       "      <td>The Village,</td>\n",
       "      <td>23 AUG 2023</td>\n",
       "      <td>7:30 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1st ODI -</td>\n",
       "      <td>1st</td>\n",
       "      <td>Pallekele International Cricket Stadium,</td>\n",
       "      <td>2 SEP 2023</td>\n",
       "      <td>10:00 AM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2nd ODI -</td>\n",
       "      <td>2nd</td>\n",
       "      <td>Pallekele International Cricket Stadium,</td>\n",
       "      <td>4 SEP 2023</td>\n",
       "      <td>10:00 AM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1st ODI -</td>\n",
       "      <td>1st</td>\n",
       "      <td>Punjab Cricket Association IS Bindra Stadium,</td>\n",
       "      <td>22 SEP 2023</td>\n",
       "      <td>1:30 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2nd ODI -</td>\n",
       "      <td>2nd</td>\n",
       "      <td>Holkar Cricket Stadium,</td>\n",
       "      <td>24 SEP 2023</td>\n",
       "      <td>1:30 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3rd ODI -</td>\n",
       "      <td>3rd</td>\n",
       "      <td>Saurashtra Cricket Association Stadium,</td>\n",
       "      <td>27 SEP 2023</td>\n",
       "      <td>1:30 PM IST</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  MATCH TITLE SERIES                                          PLACE  \\\n",
       "0  1st T20I -    1st                                   The Village,   \n",
       "1  2nd T20I -    2nd                                   The Village,   \n",
       "2  3rd T20I -    3rd                                   The Village,   \n",
       "3   1st ODI -    1st       Pallekele International Cricket Stadium,   \n",
       "4   2nd ODI -    2nd       Pallekele International Cricket Stadium,   \n",
       "5   1st ODI -    1st  Punjab Cricket Association IS Bindra Stadium,   \n",
       "6   2nd ODI -    2nd                        Holkar Cricket Stadium,   \n",
       "7   3rd ODI -    3rd        Saurashtra Cricket Association Stadium,   \n",
       "\n",
       "          DATE          TIME  \n",
       "0  18 AUG 2023   7:30 PM IST  \n",
       "1  20 AUG 2023   7:30 PM IST  \n",
       "2  23 AUG 2023   7:30 PM IST  \n",
       "3   2 SEP 2023  10:00 AM IST  \n",
       "4   4 SEP 2023  10:00 AM IST  \n",
       "5  22 SEP 2023   1:30 PM IST  \n",
       "6  24 SEP 2023   1:30 PM IST  \n",
       "7  27 SEP 2023   1:30 PM IST  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2={\"MATCH TITLE\":MATCH_TITLE,\"SERIES\":SERIES,\"PLACE\":PLACE,\"DATE\":DATE,\"TIME\":TIME}\n",
    "df2=pd.DataFrame(data2)\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2f081d",
   "metadata": {},
   "source": [
    "Q3: Scrape the details of State-wise GDP of India from statisticstime.com.\n",
    "Url = http://statisticstimes.com/\n",
    "You have to find following details: A) Rank\n",
    "B) State\n",
    "C) GSDP(18-19)- at current prices\n",
    "D) GSDP(19-20)- at current prices\n",
    "E) Share(18-19)\n",
    "F) GDP($ billion)\n",
    "Note: - From statisticstimes home page you have to reach to economy page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "31962ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome()\n",
    "driver.get(' http://statisticstimes.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e662e971",
   "metadata": {},
   "outputs": [],
   "source": [
    "Economy=driver.find_element(By.XPATH,'//div[@class=\"navbar\"]/div[2]/button')\n",
    "Economy.click()\n",
    "Economy_india=driver.find_element(By.XPATH,'//div[@class=\"navbar\"]/div[2]/div/a[3]')\n",
    "Economy_india.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b00d8950",
   "metadata": {},
   "outputs": [],
   "source": [
    "GDP_IndianStates=driver.find_element(By.XPATH,'/html/body/div[2]/div[2]/div[2]/ul/li[1]/a')\n",
    "GDP_IndianStates.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ac6e3359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 33 33 33 33 33\n"
     ]
    }
   ],
   "source": [
    "RANK=[]\n",
    "STATE=[]\n",
    "GSDP1819=[]\n",
    "GSDP1920=[]\n",
    "SHARE1819=[]\n",
    "GDP=[]\n",
    "\n",
    "rank_tag=driver.find_elements(By.XPATH,'//*[@id=\"table_id\"]/tbody/tr/td[1]')\n",
    "for i in rank_tag:\n",
    "    RANK.append(i.text)         # to scrap rank details\n",
    "    \n",
    "\n",
    "state_tag=driver.find_elements(By.XPATH,'//*[@id=\"table_id\"]/tbody/tr/td[2]') \n",
    "for i in state_tag:\n",
    "    STATE.append(i.text)       # to scrap state details   \n",
    "    \n",
    "\n",
    "gsdp1819_tag=driver.find_elements(By.XPATH,'//*[@id=\"table_id\"]/tbody/tr/td[4]')    \n",
    "for i in gsdp1819_tag:\n",
    "    GSDP1819.append(i.text)        # to scrap GSDP 18-19\n",
    "    \n",
    "\n",
    "gsdp1920_tag=driver.find_elements(By.XPATH,'//*[@id=\"table_id\"]/tbody/tr/td[3]')    \n",
    "for i in gsdp1920_tag:\n",
    "    GSDP1920.append(i.text)     # to scrap GSDP 19-20\n",
    "\n",
    "    \n",
    "share_tag=driver.find_elements(By.XPATH,'//*[@id=\"table_id\"]/tbody/tr/td[5]')\n",
    "for i in share_tag:\n",
    "    SHARE1819.append(i.text)    # to scrap SHARE(18-19 \n",
    "    \n",
    "gdp_tag=driver.find_elements(By.XPATH,'//*[@id=\"table_id\"]/tbody/tr/td[6]')\n",
    "for i in gdp_tag:\n",
    "    GDP.append(i.text)        # to scrap GDP\n",
    "\n",
    "print(len(RANK),len(STATE),len(GSDP1819),len(GSDP1920),len(SHARE1819),len(GDP))    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "df7bf179",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RANK</th>\n",
       "      <th>STATE</th>\n",
       "      <th>GSDP(18-19)</th>\n",
       "      <th>GSDP(19-20)</th>\n",
       "      <th>SHARE(18-19)</th>\n",
       "      <th>GDP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Maharashtra</td>\n",
       "      <td>2,632,792</td>\n",
       "      <td>-</td>\n",
       "      <td>13.94%</td>\n",
       "      <td>399.921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Tamil Nadu</td>\n",
       "      <td>1,630,208</td>\n",
       "      <td>1,845,853</td>\n",
       "      <td>8.63%</td>\n",
       "      <td>247.629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Uttar Pradesh</td>\n",
       "      <td>1,584,764</td>\n",
       "      <td>1,687,818</td>\n",
       "      <td>8.39%</td>\n",
       "      <td>240.726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Gujarat</td>\n",
       "      <td>1,502,899</td>\n",
       "      <td>-</td>\n",
       "      <td>7.96%</td>\n",
       "      <td>228.290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Karnataka</td>\n",
       "      <td>1,493,127</td>\n",
       "      <td>1,631,977</td>\n",
       "      <td>7.91%</td>\n",
       "      <td>226.806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>West Bengal</td>\n",
       "      <td>1,089,898</td>\n",
       "      <td>1,253,832</td>\n",
       "      <td>5.77%</td>\n",
       "      <td>165.556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Rajasthan</td>\n",
       "      <td>942,586</td>\n",
       "      <td>1,020,989</td>\n",
       "      <td>4.99%</td>\n",
       "      <td>143.179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Andhra Pradesh</td>\n",
       "      <td>862,957</td>\n",
       "      <td>972,782</td>\n",
       "      <td>4.57%</td>\n",
       "      <td>131.083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Telangana</td>\n",
       "      <td>861,031</td>\n",
       "      <td>969,604</td>\n",
       "      <td>4.56%</td>\n",
       "      <td>130.791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Madhya Pradesh</td>\n",
       "      <td>809,592</td>\n",
       "      <td>906,672</td>\n",
       "      <td>4.29%</td>\n",
       "      <td>122.977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>Kerala</td>\n",
       "      <td>781,653</td>\n",
       "      <td>-</td>\n",
       "      <td>4.14%</td>\n",
       "      <td>118.733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>774,870</td>\n",
       "      <td>856,112</td>\n",
       "      <td>4.10%</td>\n",
       "      <td>117.703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>Haryana</td>\n",
       "      <td>734,163</td>\n",
       "      <td>831,610</td>\n",
       "      <td>3.89%</td>\n",
       "      <td>111.519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>Bihar</td>\n",
       "      <td>530,363</td>\n",
       "      <td>611,804</td>\n",
       "      <td>2.81%</td>\n",
       "      <td>80.562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>Punjab</td>\n",
       "      <td>526,376</td>\n",
       "      <td>574,760</td>\n",
       "      <td>2.79%</td>\n",
       "      <td>79.957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>Odisha</td>\n",
       "      <td>487,805</td>\n",
       "      <td>521,275</td>\n",
       "      <td>2.58%</td>\n",
       "      <td>74.098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>Assam</td>\n",
       "      <td>315,881</td>\n",
       "      <td>-</td>\n",
       "      <td>1.67%</td>\n",
       "      <td>47.982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>Chhattisgarh</td>\n",
       "      <td>304,063</td>\n",
       "      <td>329,180</td>\n",
       "      <td>1.61%</td>\n",
       "      <td>46.187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>Jharkhand</td>\n",
       "      <td>297,204</td>\n",
       "      <td>328,598</td>\n",
       "      <td>1.57%</td>\n",
       "      <td>45.145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>Uttarakhand</td>\n",
       "      <td>245,895</td>\n",
       "      <td>-</td>\n",
       "      <td>1.30%</td>\n",
       "      <td>37.351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>Jammu &amp; Kashmir</td>\n",
       "      <td>155,956</td>\n",
       "      <td>-</td>\n",
       "      <td>0.83%</td>\n",
       "      <td>23.690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>Himachal Pradesh</td>\n",
       "      <td>153,845</td>\n",
       "      <td>165,472</td>\n",
       "      <td>0.81%</td>\n",
       "      <td>23.369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>Goa</td>\n",
       "      <td>73,170</td>\n",
       "      <td>80,449</td>\n",
       "      <td>0.39%</td>\n",
       "      <td>11.115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>Tripura</td>\n",
       "      <td>49,845</td>\n",
       "      <td>55,984</td>\n",
       "      <td>0.26%</td>\n",
       "      <td>7.571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>Chandigarh</td>\n",
       "      <td>42,114</td>\n",
       "      <td>-</td>\n",
       "      <td>0.22%</td>\n",
       "      <td>6.397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>Puducherry</td>\n",
       "      <td>34,433</td>\n",
       "      <td>38,253</td>\n",
       "      <td>0.18%</td>\n",
       "      <td>5.230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>Meghalaya</td>\n",
       "      <td>33,481</td>\n",
       "      <td>36,572</td>\n",
       "      <td>0.18%</td>\n",
       "      <td>5.086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>Sikkim</td>\n",
       "      <td>28,723</td>\n",
       "      <td>32,496</td>\n",
       "      <td>0.15%</td>\n",
       "      <td>4.363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>Manipur</td>\n",
       "      <td>27,870</td>\n",
       "      <td>31,790</td>\n",
       "      <td>0.15%</td>\n",
       "      <td>4.233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>Nagaland</td>\n",
       "      <td>27,283</td>\n",
       "      <td>-</td>\n",
       "      <td>0.14%</td>\n",
       "      <td>4.144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31</td>\n",
       "      <td>Arunachal Pradesh</td>\n",
       "      <td>24,603</td>\n",
       "      <td>-</td>\n",
       "      <td>0.13%</td>\n",
       "      <td>3.737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>Mizoram</td>\n",
       "      <td>22,287</td>\n",
       "      <td>26,503</td>\n",
       "      <td>0.12%</td>\n",
       "      <td>3.385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>33</td>\n",
       "      <td>Andaman &amp; Nicobar Islands</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RANK                      STATE GSDP(18-19) GSDP(19-20) SHARE(18-19)  \\\n",
       "0     1                Maharashtra   2,632,792           -       13.94%   \n",
       "1     2                 Tamil Nadu   1,630,208   1,845,853        8.63%   \n",
       "2     3              Uttar Pradesh   1,584,764   1,687,818        8.39%   \n",
       "3     4                    Gujarat   1,502,899           -        7.96%   \n",
       "4     5                  Karnataka   1,493,127   1,631,977        7.91%   \n",
       "5     6                West Bengal   1,089,898   1,253,832        5.77%   \n",
       "6     7                  Rajasthan     942,586   1,020,989        4.99%   \n",
       "7     8             Andhra Pradesh     862,957     972,782        4.57%   \n",
       "8     9                  Telangana     861,031     969,604        4.56%   \n",
       "9    10             Madhya Pradesh     809,592     906,672        4.29%   \n",
       "10   11                     Kerala     781,653           -        4.14%   \n",
       "11   12                      Delhi     774,870     856,112        4.10%   \n",
       "12   13                    Haryana     734,163     831,610        3.89%   \n",
       "13   14                      Bihar     530,363     611,804        2.81%   \n",
       "14   15                     Punjab     526,376     574,760        2.79%   \n",
       "15   16                     Odisha     487,805     521,275        2.58%   \n",
       "16   17                      Assam     315,881           -        1.67%   \n",
       "17   18               Chhattisgarh     304,063     329,180        1.61%   \n",
       "18   19                  Jharkhand     297,204     328,598        1.57%   \n",
       "19   20                Uttarakhand     245,895           -        1.30%   \n",
       "20   21            Jammu & Kashmir     155,956           -        0.83%   \n",
       "21   22           Himachal Pradesh     153,845     165,472        0.81%   \n",
       "22   23                        Goa      73,170      80,449        0.39%   \n",
       "23   24                    Tripura      49,845      55,984        0.26%   \n",
       "24   25                 Chandigarh      42,114           -        0.22%   \n",
       "25   26                 Puducherry      34,433      38,253        0.18%   \n",
       "26   27                  Meghalaya      33,481      36,572        0.18%   \n",
       "27   28                     Sikkim      28,723      32,496        0.15%   \n",
       "28   29                    Manipur      27,870      31,790        0.15%   \n",
       "29   30                   Nagaland      27,283           -        0.14%   \n",
       "30   31          Arunachal Pradesh      24,603           -        0.13%   \n",
       "31   32                    Mizoram      22,287      26,503        0.12%   \n",
       "32   33  Andaman & Nicobar Islands           -           -            -   \n",
       "\n",
       "        GDP  \n",
       "0   399.921  \n",
       "1   247.629  \n",
       "2   240.726  \n",
       "3   228.290  \n",
       "4   226.806  \n",
       "5   165.556  \n",
       "6   143.179  \n",
       "7   131.083  \n",
       "8   130.791  \n",
       "9   122.977  \n",
       "10  118.733  \n",
       "11  117.703  \n",
       "12  111.519  \n",
       "13   80.562  \n",
       "14   79.957  \n",
       "15   74.098  \n",
       "16   47.982  \n",
       "17   46.187  \n",
       "18   45.145  \n",
       "19   37.351  \n",
       "20   23.690  \n",
       "21   23.369  \n",
       "22   11.115  \n",
       "23    7.571  \n",
       "24    6.397  \n",
       "25    5.230  \n",
       "26    5.086  \n",
       "27    4.363  \n",
       "28    4.233  \n",
       "29    4.144  \n",
       "30    3.737  \n",
       "31    3.385  \n",
       "32        -  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data3={\"RANK\":RANK,\"STATE\":STATE,\"GSDP(18-19)\":GSDP1819,\"GSDP(19-20)\":GSDP1920,\"SHARE(18-19)\":SHARE1819,\"GDP\":GDP}\n",
    "df3=pd.DataFrame(data3)\n",
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576e3f03",
   "metadata": {},
   "source": [
    "4. Scrape the details of trending repositories on Github.com.\n",
    "Url = https://github.com/\n",
    "You have to find the following details:\n",
    "A) Repository title\n",
    "B) Repository description\n",
    "C) Contributors count\n",
    "D) Language used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "585aeff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome()\n",
    "driver.get(' https://github.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "086771af",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories=driver.find_element(By.XPATH,'//span[@class=\"Button-content\"]')\n",
    "categories.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68e38e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "open_source=driver.find_element(By.XPATH,'/html/body/div[1]/div[1]/header/div/div[2]/div/nav/ul/li[3]/button')\n",
    "open_source.click()\n",
    "trendingRepository=driver.find_element(By.XPATH,'/html/body/div[1]/div[1]/header/div/div[2]/div/nav/ul/li[3]/div/div[3]/ul/li[2]/a')\n",
    "trendingRepository.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a916c2bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GITHUB_URL=[]\n",
    "url=driver.find_elements(By.XPATH,'//a[@class=\"Link\"]')\n",
    "for i in url:\n",
    "    GITHUB_URL.append(i.get_attribute('href'))\n",
    "len(GITHUB_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b0e728b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GITHUB URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://github.com/getumbrel/llama-gpt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://github.com/qiuyu96/CoDeF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://github.com/leejet/stable-diffusion.cpp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://github.com/jackfrued/Python-100-Days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://github.com/LorisYounger/VPet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://github.com/kuafuai/DevOpsGPT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://github.com/calcom/cal.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>https://github.com/Grasscutters/Grasscutter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>https://github.com/vinta/awesome-python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>https://github.com/dimdenGD/OldTweetDeck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>https://github.com/serde-rs/serde</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>https://github.com/ill-inc/biomes-game</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>https://github.com/facebookresearch/AnimatedDr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>https://github.com/elidianaandrade/dio-lab-ope...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>https://github.com/documenso/documenso</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>https://github.com/microsoft/Bringing-Old-Phot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>https://github.com/organicmaps/organicmaps</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>https://github.com/OpenBB-finance/OpenBBTerminal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>https://github.com/EbookFoundation/free-progra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>https://github.com/smol-ai/GodMode</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>https://github.com/jiran214/GPT-vup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>https://github.com/hehonghui/awesome-english-e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>https://github.com/roboflow/supervision</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>https://github.com/aseprite/aseprite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>https://github.com/StanGirard/quivr</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           GITHUB URL\n",
       "0              https://github.com/getumbrel/llama-gpt\n",
       "1                    https://github.com/qiuyu96/CoDeF\n",
       "2      https://github.com/leejet/stable-diffusion.cpp\n",
       "3        https://github.com/jackfrued/Python-100-Days\n",
       "4                https://github.com/LorisYounger/VPet\n",
       "5                https://github.com/kuafuai/DevOpsGPT\n",
       "6                   https://github.com/calcom/cal.com\n",
       "7         https://github.com/Grasscutters/Grasscutter\n",
       "8             https://github.com/vinta/awesome-python\n",
       "9            https://github.com/dimdenGD/OldTweetDeck\n",
       "10                  https://github.com/serde-rs/serde\n",
       "11             https://github.com/ill-inc/biomes-game\n",
       "12  https://github.com/facebookresearch/AnimatedDr...\n",
       "13  https://github.com/elidianaandrade/dio-lab-ope...\n",
       "14             https://github.com/documenso/documenso\n",
       "15  https://github.com/microsoft/Bringing-Old-Phot...\n",
       "16         https://github.com/organicmaps/organicmaps\n",
       "17   https://github.com/OpenBB-finance/OpenBBTerminal\n",
       "18  https://github.com/EbookFoundation/free-progra...\n",
       "19                 https://github.com/smol-ai/GodMode\n",
       "20                https://github.com/jiran214/GPT-vup\n",
       "21  https://github.com/hehonghui/awesome-english-e...\n",
       "22            https://github.com/roboflow/supervision\n",
       "23               https://github.com/aseprite/aseprite\n",
       "24                https://github.com/StanGirard/quivr"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1={\"GITHUB URL\":GITHUB_URL}\n",
    "DF=pd.DataFrame(d1)\n",
    "DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "43e1e043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    }
   ],
   "source": [
    "CONTRIBUTORS_COUNT=[]\n",
    "for url in GITHUB_URL:\n",
    "    driver.get(url)\n",
    "    time.sleep(5)\n",
    "    try:                                                                     # to scrap contributors count\n",
    "        contributors_tag=driver.find_elements(By.XPATH,'//div[@class=\"Layout-sidebar\"][1]/div[1]/div[3]/div[1]/h2[1]/a[1]/span[1]')\n",
    "        for i in contributors_tag:\n",
    "            CONTRIBUTORS_COUNT.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "         CONTRIBUTORS_COUNT.append('-')    \n",
    "print(len(CONTRIBUTORS_COUNT))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b413286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 25 18 16\n"
     ]
    }
   ],
   "source": [
    "\n",
    "REPOSITORY_TITLE=[]\n",
    "REPOSITORY_DESCRIPTION=[]\n",
    "CONTRIBUTORS_COUNT=[]\n",
    "LANGUAGE=[]\n",
    "\n",
    "for url in GITHUB_URL:\n",
    "    driver.get(url)\n",
    "    time.sleep(5)\n",
    "        \n",
    "    try:                                                                      # for scraping repository_title\n",
    "    \n",
    "        title_tag=driver.find_elements(By.XPATH,'//span[@class=\"author flex-self-stretch\"][1]/a[1]')\n",
    "        for i in title_tag:\n",
    "            REPOSITORY_TITLE.append(i.text)        \n",
    "    except NoSuchElementException:\n",
    "        REPOSITORY_TITLE.append('-')\n",
    "    \n",
    "    try:                                                                     #for scraping repository description\n",
    "        description_tag=driver.find_elements(By.XPATH,'//div[@class=\"Layout-sidebar\"][1]/div/div/div/p[1]')  \n",
    "        for i in description_tag:\n",
    "            REPOSITORY_DESCRIPTION.append(i.text)         \n",
    "    except NoSuchElementException:\n",
    "        REPOSITORY_DESCRIPTION.append('-')\n",
    "    \n",
    "    try:                                                                     # to scrap contributors count\n",
    "        contributors_tag=driver.find_elements(By.XPATH,'//div[@class=\"Layout-sidebar\"]//div[1]//div[3]/div[1]//h2[1]//a[1]//span[1]')\n",
    "        for i in contributors_tag:\n",
    "            CONTRIBUTORS_COUNT.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        CONTRIBUTORS_COUNT.append('-')\n",
    "    # to scrap languages used\n",
    "    try: \n",
    "        language_tag=driver.find_elements(By.XPATH,'//div[@class=\"Layout-sidebar\"]//div[1]//div[4]//ul[1]')\n",
    "        for i in language_tag:\n",
    "            LANGUAGE.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        LANGUAGE.append('-')\n",
    "    \n",
    "print(len(REPOSITORY_TITLE),len(REPOSITORY_DESCRIPTION),len(CONTRIBUTORS_COUNT),len(LANGUAGE))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6470b0ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>REPOSITORY TITLE</th>\n",
       "      <th>REPOSITORY DESCRIPTION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [REPOSITORY TITLE, REPOSITORY DESCRIPTION]\n",
       "Index: []"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data4={\"REPOSITORY TITLE\":REPOSITORY_TITLE,\"REPOSITORY DESCRIPTION\":REPOSITORY_DESCRIPTION,\"CONTRIBUTORS COUNT\":CONTRIBUTORS_COUNT,\"LANGUAGE\":LANGUAGE}\n",
    "df4=pd.DataFrame(data4)\n",
    "df4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554b4ce7",
   "metadata": {},
   "source": [
    "5. Scrape the details of top 100 songs on billiboard.com. Url = https:/www.billboard.com/\n",
    "You have to find the following details:\n",
    "A) Song name\n",
    "B) Artist name\n",
    "C) Last week rank\n",
    "D) Peak rank\n",
    "E) Weeks on board\n",
    "Note: - From the home page you have to click on the charts option then hot 100-page link through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b11c20e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome()\n",
    "driver.get('https://www.billboard.com/charts/')\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ca14a7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "charts=driver.find_element(By.XPATH,'//div[@class=\"lrv-u-flex lrv-u-justify-content-center\"]/a')\n",
    "charts.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b2760950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100 100 100 100\n"
     ]
    }
   ],
   "source": [
    "SONG_NAME=[]\n",
    "ARTIST_NAME=[]\n",
    "LASTWEEK_RANK=[]\n",
    "PEAK_RANK=[]\n",
    "WEEKS_ON_BOARD=[]\n",
    "\n",
    "#for scraping song name\n",
    "try:\n",
    "    name_tag=driver.find_elements(By.XPATH,'//li[@class=\"lrv-u-width-100p\"]/ul/li[1]/h3')\n",
    "    for i in name_tag:\n",
    "        SONG_NAME.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    SONG_NAME.append('-')\n",
    "    \n",
    "#for scraping artist name:\n",
    "\n",
    "try:\n",
    "    artist_tag=driver.find_elements(By.XPATH,'//li[@class=\"lrv-u-width-100p\"]/ul/li[1]/span')\n",
    "    for i in artist_tag:\n",
    "        ARTIST_NAME.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    ARTIST_NAME.append('-')\n",
    "    \n",
    "#for scraping lastweek rank:\n",
    "try:\n",
    "    lastweek_tag=driver.find_elements(By.XPATH,'//li[@class=\"lrv-u-width-100p\"]/ul/li[4]/span')\n",
    "    for i in lastweek_tag:\n",
    "        LASTWEEK_RANK.append(i.text)\n",
    "except NoSuchElementException:   \n",
    "    LASTWEEK_RANK.append('-')\n",
    "    \n",
    " #for scraping peakrank:\n",
    "try:\n",
    "    peak_rank=driver.find_elements(By.XPATH,'//li[@class=\"lrv-u-width-100p\"]/ul/li[5]/span')\n",
    "    for i in peak_rank:\n",
    "            PEAK_RANK.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    PEAK_RANK.append('-')\n",
    "    \n",
    "#for scraping weeks on board\n",
    "try:\n",
    "    weeks_tag=driver.find_elements(By.XPATH,'//li[@class=\"lrv-u-width-100p\"]/ul/li[6]/span')\n",
    "    for i in weeks_tag:\n",
    "        WEEKS_ON_BOARD.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    WEEKS_ON_BOARD.append('-')\n",
    "\n",
    "print(len(SONG_NAME),len(ARTIST_NAME),len(LASTWEEK_RANK),len(PEAK_RANK),len(WEEKS_ON_BOARD))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3b515140",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SONG NAME</th>\n",
       "      <th>ARTIST NAME</th>\n",
       "      <th>LASTWEEK RANK</th>\n",
       "      <th>PEAK RANK</th>\n",
       "      <th>WEEKS_ON_BOARD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Last Night</td>\n",
       "      <td>Morgan Wallen</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fast Car</td>\n",
       "      <td>Luke Combs</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cruel Summer</td>\n",
       "      <td>Taylor Swift</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Calm Down</td>\n",
       "      <td>Rema &amp; Selena Gomez</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fukumean</td>\n",
       "      <td>Gunna</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Lagunas</td>\n",
       "      <td>Peso Pluma &amp; Jasiel Nunez</td>\n",
       "      <td>-</td>\n",
       "      <td>77</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Overdrive</td>\n",
       "      <td>Post Malone</td>\n",
       "      <td>68</td>\n",
       "      <td>47</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Bzrp Music Sessions, Vol. 55</td>\n",
       "      <td>Bizarrap &amp; Peso Pluma</td>\n",
       "      <td>99</td>\n",
       "      <td>31</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Dawns</td>\n",
       "      <td>Zach Bryan Featuring Maggie Rogers</td>\n",
       "      <td>-</td>\n",
       "      <td>42</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Rubicon</td>\n",
       "      <td>Peso Pluma</td>\n",
       "      <td>-</td>\n",
       "      <td>63</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       SONG NAME                         ARTIST NAME  \\\n",
       "0                     Last Night                       Morgan Wallen   \n",
       "1                       Fast Car                          Luke Combs   \n",
       "2                   Cruel Summer                        Taylor Swift   \n",
       "3                      Calm Down                 Rema & Selena Gomez   \n",
       "4                       Fukumean                               Gunna   \n",
       "..                           ...                                 ...   \n",
       "95                       Lagunas           Peso Pluma & Jasiel Nunez   \n",
       "96                     Overdrive                         Post Malone   \n",
       "97  Bzrp Music Sessions, Vol. 55               Bizarrap & Peso Pluma   \n",
       "98                         Dawns  Zach Bryan Featuring Maggie Rogers   \n",
       "99                       Rubicon                          Peso Pluma   \n",
       "\n",
       "   LASTWEEK RANK PEAK RANK WEEKS_ON_BOARD  \n",
       "0              1         1             28  \n",
       "1              2         2             20  \n",
       "2              4         3             14  \n",
       "3              6         3             49  \n",
       "4              7         4              8  \n",
       "..           ...       ...            ...  \n",
       "95             -        77              6  \n",
       "96            68        47              3  \n",
       "97            99        31             10  \n",
       "98             -        42             15  \n",
       "99             -        63              6  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data5={\"SONG NAME\":SONG_NAME,\"ARTIST NAME\":ARTIST_NAME,\"LASTWEEK RANK\":LASTWEEK_RANK,\"PEAK RANK\":PEAK_RANK,\"WEEKS_ON_BOARD\":WEEKS_ON_BOARD}\n",
    "df5=pd.DataFrame(data5)\n",
    "df5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19692057",
   "metadata": {},
   "source": [
    "Q6: Url =\n",
    "https://www.theguardian.com/news/datablog/2012/aug/09/best\n",
    "-\n",
    "selling\n",
    "-\n",
    "books\n",
    "-\n",
    "all\n",
    "-\n",
    "time\n",
    "-\n",
    "fifty\n",
    "-\n",
    "shades\n",
    "-\n",
    "grey\n",
    "-\n",
    "You have to find the following details:\n",
    "Scrape the details of Highest selling novels.\n",
    "compare\n",
    "A) Book name\n",
    "B) Author name\n",
    "C) Volumes sold\n",
    "D) Publisher\n",
    "E) Genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7236e787",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome()\n",
    "driver.get('https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95ec5636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100 100 100 100\n"
     ]
    }
   ],
   "source": [
    "BOOK_NAME=[]\n",
    "AUTHOR_NAME=[]\n",
    "VOLUMES_SOLD=[]\n",
    "PUBLISHER=[]\n",
    "GENRE=[]\n",
    "# for scraping names\n",
    "try:\n",
    "    name_tag=driver.find_elements(By.XPATH,'//div[@class=\"embed block\"][1]/table/tbody/tr/td[2]')\n",
    "    for i in name_tag:\n",
    "        BOOK_NAME.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    BOOK_NAME.append('-')\n",
    "    \n",
    "#for scraping Author name\n",
    "try:\n",
    "    author_tag=driver.find_elements(By.XPATH,'//div[@class=\"embed block\"][1]/table/tbody/tr/td[3]')\n",
    "    for i in author_tag:\n",
    "        AUTHOR_NAME.append(i.text)\n",
    "        \n",
    "except NoSuchElementException:\n",
    "    AUTHOR_NAME.append('-')\n",
    "    \n",
    "#for scraping volumes sold:\n",
    "try:\n",
    "    volumes_tag=driver.find_elements(By.XPATH,'//div[@class=\"embed block\"][1]/table/tbody/tr/td[4]')\n",
    "    for i in volumes_tag:\n",
    "        VOLUMES_SOLD.append(i.text)\n",
    "except NoSuchElementException:    \n",
    "    VOLUMES_SOLD.append('-')\n",
    "    \n",
    "#for scraping publisher\n",
    "try:\n",
    "    publisher_tag=driver.find_elements(By.XPATH,'//div[@class=\"embed block\"][1]/table/tbody/tr/td[5]')\n",
    "    for i in publisher_tag:\n",
    "        PUBLISHER.append(i.text)\n",
    "except NoSuchElementException:    \n",
    "    PUBLISHER.append('-')\n",
    "    \n",
    "# for scraping genre:\n",
    "try:\n",
    "    genre_tag=driver.find_elements(By.XPATH,'//div[@class=\"embed block\"][1]/table/tbody/tr/td[6]')\n",
    "    for i in genre_tag:\n",
    "        GENRE.append(i.text)\n",
    "except NoSuchElementException:   \n",
    "    GENRE.append('-')\n",
    "    \n",
    "print(len(BOOK_NAME),len(AUTHOR_NAME),len(VOLUMES_SOLD),len(PUBLISHER),len(GENRE))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "664ca333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BOOK NAME</th>\n",
       "      <th>AUTHOR NAME</th>\n",
       "      <th>VOLUMES SOLD</th>\n",
       "      <th>PUBLISHER</th>\n",
       "      <th>GENRE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Da Vinci Code,The</td>\n",
       "      <td>Brown, Dan</td>\n",
       "      <td>5,094,805</td>\n",
       "      <td>Transworld</td>\n",
       "      <td>Crime, Thriller &amp; Adventure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Harry Potter and the Deathly Hallows</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>4,475,152</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Children's Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Harry Potter and the Philosopher's Stone</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>4,200,654</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Children's Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Harry Potter and the Order of the Phoenix</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>4,179,479</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Children's Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fifty Shades of Grey</td>\n",
       "      <td>James, E. L.</td>\n",
       "      <td>3,758,936</td>\n",
       "      <td>Random House</td>\n",
       "      <td>Romance &amp; Sagas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Ghost,The</td>\n",
       "      <td>Harris, Robert</td>\n",
       "      <td>807,311</td>\n",
       "      <td>Random House</td>\n",
       "      <td>General &amp; Literary Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Happy Days with the Naked Chef</td>\n",
       "      <td>Oliver, Jamie</td>\n",
       "      <td>794,201</td>\n",
       "      <td>Penguin</td>\n",
       "      <td>Food &amp; Drink: General</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Hunger Games,The:Hunger Games Trilogy</td>\n",
       "      <td>Collins, Suzanne</td>\n",
       "      <td>792,187</td>\n",
       "      <td>Scholastic Ltd.</td>\n",
       "      <td>Young Adult Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Lost Boy,The:A Foster Child's Search for the L...</td>\n",
       "      <td>Pelzer, Dave</td>\n",
       "      <td>791,507</td>\n",
       "      <td>Orion</td>\n",
       "      <td>Biography: General</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Jamie's Ministry of Food:Anyone Can Learn to C...</td>\n",
       "      <td>Oliver, Jamie</td>\n",
       "      <td>791,095</td>\n",
       "      <td>Penguin</td>\n",
       "      <td>Food &amp; Drink: General</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            BOOK NAME       AUTHOR NAME  \\\n",
       "0                                   Da Vinci Code,The        Brown, Dan   \n",
       "1                Harry Potter and the Deathly Hallows     Rowling, J.K.   \n",
       "2            Harry Potter and the Philosopher's Stone     Rowling, J.K.   \n",
       "3           Harry Potter and the Order of the Phoenix     Rowling, J.K.   \n",
       "4                                Fifty Shades of Grey      James, E. L.   \n",
       "..                                                ...               ...   \n",
       "95                                          Ghost,The    Harris, Robert   \n",
       "96                     Happy Days with the Naked Chef     Oliver, Jamie   \n",
       "97              Hunger Games,The:Hunger Games Trilogy  Collins, Suzanne   \n",
       "98  Lost Boy,The:A Foster Child's Search for the L...      Pelzer, Dave   \n",
       "99  Jamie's Ministry of Food:Anyone Can Learn to C...     Oliver, Jamie   \n",
       "\n",
       "   VOLUMES SOLD        PUBLISHER                        GENRE  \n",
       "0     5,094,805       Transworld  Crime, Thriller & Adventure  \n",
       "1     4,475,152       Bloomsbury           Children's Fiction  \n",
       "2     4,200,654       Bloomsbury           Children's Fiction  \n",
       "3     4,179,479       Bloomsbury           Children's Fiction  \n",
       "4     3,758,936     Random House              Romance & Sagas  \n",
       "..          ...              ...                          ...  \n",
       "95      807,311     Random House   General & Literary Fiction  \n",
       "96      794,201          Penguin        Food & Drink: General  \n",
       "97      792,187  Scholastic Ltd.          Young Adult Fiction  \n",
       "98      791,507            Orion           Biography: General  \n",
       "99      791,095          Penguin        Food & Drink: General  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data6={\"BOOK NAME\":BOOK_NAME,\"AUTHOR NAME\":AUTHOR_NAME,\"VOLUMES SOLD\":VOLUMES_SOLD,\"PUBLISHER\":PUBLISHER,\"GENRE\":GENRE}\n",
    "df6=pd.DataFrame(data6)\n",
    "df6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7316854e",
   "metadata": {},
   "source": [
    "Q7: Scrape the details most watched tv series of all time from imdb.com.\n",
    "Url = https://www.imdb.com/list/ls095964455/ You have to find the following details:\n",
    "A) Name\n",
    "B) Year span\n",
    "C) Genre\n",
    "D) Run time\n",
    "E) Ratings\n",
    "F) Votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63e96789",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome()\n",
    "driver.get('https://www.imdb.com/list/ls095964455/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "638d66b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100 100 100 100 100\n"
     ]
    }
   ],
   "source": [
    "NAME=[]\n",
    "YEAR_SPAN=[]\n",
    "GENRE=[]\n",
    "RUN_TIME=[]\n",
    "RATINGS=[]\n",
    "VOTES=[]\n",
    "\n",
    "# for scraping names of tv shows\n",
    "try:\n",
    "    name_tag=driver.find_elements(By.XPATH,'//div[@class=\"lister-list\"][1]/div/div[2]/h3/a')\n",
    "    for i in name_tag:\n",
    "        NAME.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    NAME.append('-')\n",
    "\n",
    "# for scraping year span    \n",
    "try:\n",
    "    year_tag=driver.find_elements(By.XPATH,'//div[@class=\"lister-list\"][1]/div/div[2]/h3/span[2]')\n",
    "    for i in year_tag:\n",
    "        YEAR_SPAN.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    YEAR_SPAN.append('-')  \n",
    "    \n",
    "# for scraping GENRE:    \n",
    "try:\n",
    "    genre_tag=driver.find_elements(By.XPATH,'//div[@class=\"lister-list\"][1]/div/div[2]/p[1]/span[5]')\n",
    "    for i in genre_tag:\n",
    "        GENRE.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    GENRE.append('-')\n",
    "    \n",
    "#for scraping run time:\n",
    "try:\n",
    "    runtime_tag=driver.find_elements(By.XPATH,'//div[@class=\"lister-list\"][1]/div/div[2]/p[1]/span[3]')\n",
    "    for i in runtime_tag:\n",
    "        RUN_TIME.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    RUN_TIME.append('-')\n",
    "    \n",
    "#for scraping ratings:\n",
    "try:\n",
    "    ratings_tag=driver.find_elements(By.XPATH,'//div[@class=\"lister-list\"][1]/div/div[2]/div/div/span[2]')\n",
    "    for i in ratings_tag:\n",
    "        RATINGS.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    RATINGS.append('-')\n",
    "\n",
    "#for scraping votes:\n",
    "try:\n",
    "    votes_tag=driver.find_elements(By.XPATH,'//div[@class=\"lister-list\"][1]/div/div[2]/p[4]/span[2]')\n",
    "    for i in votes_tag:\n",
    "        VOTES.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    VOTES.append('-')\n",
    "    \n",
    "print(len(NAME),len(YEAR_SPAN),len(GENRE),len(RUN_TIME),len(RATINGS),len(VOTES))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2164245b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NAME</th>\n",
       "      <th>YEAR SPAN</th>\n",
       "      <th>RUN TIME</th>\n",
       "      <th>RATINGS</th>\n",
       "      <th>VOTES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Game of Thrones</td>\n",
       "      <td>(2011â€“2019)</td>\n",
       "      <td>57 min</td>\n",
       "      <td>9.2</td>\n",
       "      <td>2,193,366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stranger Things</td>\n",
       "      <td>(2016â€“2024)</td>\n",
       "      <td>51 min</td>\n",
       "      <td>8.7</td>\n",
       "      <td>1,267,099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Walking Dead</td>\n",
       "      <td>(2010â€“2022)</td>\n",
       "      <td>44 min</td>\n",
       "      <td>8.1</td>\n",
       "      <td>1,041,100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13 Reasons Why</td>\n",
       "      <td>(2017â€“2020)</td>\n",
       "      <td>60 min</td>\n",
       "      <td>7.5</td>\n",
       "      <td>305,991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The 100</td>\n",
       "      <td>(2014â€“2020)</td>\n",
       "      <td>43 min</td>\n",
       "      <td>7.6</td>\n",
       "      <td>265,074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Reign</td>\n",
       "      <td>(2013â€“2017)</td>\n",
       "      <td>42 min</td>\n",
       "      <td>7.4</td>\n",
       "      <td>52,435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>A Series of Unfortunate Events</td>\n",
       "      <td>(2017â€“2019)</td>\n",
       "      <td>50 min</td>\n",
       "      <td>7.8</td>\n",
       "      <td>64,462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Criminal Minds</td>\n",
       "      <td>(2005â€“ )</td>\n",
       "      <td>42 min</td>\n",
       "      <td>8.1</td>\n",
       "      <td>210,123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Scream: The TV Series</td>\n",
       "      <td>(2015â€“2019)</td>\n",
       "      <td>45 min</td>\n",
       "      <td>7</td>\n",
       "      <td>43,693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>The Haunting of Hill House</td>\n",
       "      <td>(2018)</td>\n",
       "      <td>572 min</td>\n",
       "      <td>8.6</td>\n",
       "      <td>263,707</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              NAME    YEAR SPAN RUN TIME RATINGS      VOTES\n",
       "0                  Game of Thrones  (2011â€“2019)   57 min     9.2  2,193,366\n",
       "1                  Stranger Things  (2016â€“2024)   51 min     8.7  1,267,099\n",
       "2                 The Walking Dead  (2010â€“2022)   44 min     8.1  1,041,100\n",
       "3                   13 Reasons Why  (2017â€“2020)   60 min     7.5    305,991\n",
       "4                          The 100  (2014â€“2020)   43 min     7.6    265,074\n",
       "..                             ...          ...      ...     ...        ...\n",
       "95                           Reign  (2013â€“2017)   42 min     7.4     52,435\n",
       "96  A Series of Unfortunate Events  (2017â€“2019)   50 min     7.8     64,462\n",
       "97                  Criminal Minds     (2005â€“ )   42 min     8.1    210,123\n",
       "98           Scream: The TV Series  (2015â€“2019)   45 min       7     43,693\n",
       "99      The Haunting of Hill House       (2018)  572 min     8.6    263,707\n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data7={\"NAME\":NAME,\"YEAR SPAN\":YEAR_SPAN,\"RUN TIME\":RUN_TIME,\"RATINGS\":RATINGS,\"VOTES\":VOTES}\n",
    "df7=pd.DataFrame(data7)\n",
    "df7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8db1a2",
   "metadata": {},
   "source": [
    "Q8: Details of Datasets from UCI machine learning repositories.\n",
    "Url = https://archive.ics.uci.edu/\n",
    "You have to find the following details:\n",
    "A) Dataset name\n",
    "B) Data type\n",
    "C) Task\n",
    "D) Attribute type\n",
    "E) No of instances\n",
    "F) No of attribute G) Year\n",
    "Note: - from the home page you have to go to the Show All Dataset page through code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "94cfe7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome()\n",
    "driver.get('https://archive.ics.uci.edu/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d16fcc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "view_dataset=driver.find_element(By.XPATH,'/html/body/div/div[1]/div[1]/main/div/div[1]/div/div/div/a[1]')\n",
    "view_dataset.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b52df728",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASET_URL=[]\n",
    "url=driver.find_elements(By.XPATH,'//a[@class=\"link-hover link text-xl font-semibold\"]')\n",
    "for i in url:\n",
    "    DATASET_URL.append(i.get_attribute('href'))\n",
    "len(DATASET_URL)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "08705d3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATAURL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://archive.ics.uci.edu/dataset/53/iris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://archive.ics.uci.edu/dataset/45/heart+d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://archive.ics.uci.edu/dataset/2/adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://archive.ics.uci.edu/dataset/602/dry+be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://archive.ics.uci.edu/dataset/34/diabetes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://archive.ics.uci.edu/dataset/109/wine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://archive.ics.uci.edu/dataset/17/breast+...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>https://archive.ics.uci.edu/dataset/545/rice+c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>https://archive.ics.uci.edu/dataset/19/car+eva...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>https://archive.ics.uci.edu/dataset/73/mushroom</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             DATAURL\n",
       "0        https://archive.ics.uci.edu/dataset/53/iris\n",
       "1  https://archive.ics.uci.edu/dataset/45/heart+d...\n",
       "2        https://archive.ics.uci.edu/dataset/2/adult\n",
       "3  https://archive.ics.uci.edu/dataset/602/dry+be...\n",
       "4    https://archive.ics.uci.edu/dataset/34/diabetes\n",
       "5       https://archive.ics.uci.edu/dataset/109/wine\n",
       "6  https://archive.ics.uci.edu/dataset/17/breast+...\n",
       "7  https://archive.ics.uci.edu/dataset/545/rice+c...\n",
       "8  https://archive.ics.uci.edu/dataset/19/car+eva...\n",
       "9    https://archive.ics.uci.edu/dataset/73/mushroom"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D2={\"DATAURL\":DATASET_URL}\n",
    "df9=pd.DataFrame(D2)\n",
    "df9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9e9834f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10 10 10 10\n"
     ]
    }
   ],
   "source": [
    "DATASET_NAME=[]\n",
    "DATA_TYPE=[]\n",
    "TASK=[]\n",
    "ATTRIBUTE_TYPE=[]\n",
    "NO_INSTANCES=[]\n",
    "NO_ATTRIBUTES=[]\n",
    "YEAR=[]\n",
    "for url in DATASET_URL:\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "    # to scrap dataset name\n",
    "    try:\n",
    "        name_tag=driver.find_elements(By.XPATH,'//div[@class=\"relative flex flex w-full items-center gap-4 bg-primary p-2\"][1]/div[2]/div[1]/h1[1]')\n",
    "        for i in name_tag:\n",
    "            DATASET_NAME.append(i.text)\n",
    "    except NoSuchEementException:\n",
    "        DATASET_NAME.append('-')\n",
    "        \n",
    "    # to scrap data type\n",
    "    try:\n",
    "        type_tag=driver.find_elements(By.XPATH,'//div[@class=\"relative flex flex-col gap-4 bg-base-100 p-4 shadow\"]/div[2]/div[1]/p[1]')\n",
    "        for i in type_tag:\n",
    "            DATA_TYPE.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        DATA_TYPE.append('-')      \n",
    "    #for scraping task:\n",
    "    try:\n",
    "        task_tag=driver.find_elements(By.XPATH,'//div[@class=\"relative flex flex-col gap-4 bg-base-100 p-4 shadow\"][1]/div[2]/div[3]/p[1]')\n",
    "        for i in task_tag:\n",
    "            TASK.append(i.text)\n",
    "    except NoSuchEementException:\n",
    "        TASK.append('-')\n",
    "     # for scraping attribute type\n",
    "    try:\n",
    "        attribute_type=driver.find_elements(By.XPATH,'//div[@class=\"relative flex flex-col gap-4 bg-base-100 p-4 shadow\"][1]/div[2]/div[4]/p[1]')\n",
    "        for i in attribute_type:\n",
    "            ATTRIBUTE_TYPE.append(i.text)\n",
    "    except NoSuchEementException:\n",
    "        ATTRIBUTE_TYPE.append('-')\n",
    "     # for scraping no of instances\n",
    "    try:\n",
    "        instance_tag=driver.find_elements(By.XPATH,'//div[@class=\"relative flex flex-col gap-4 bg-base-100 p-4 shadow\"][1]/div[2]/div[5]/p[1]')\n",
    "        for i in instance_tag:\n",
    "            NO_INSTANCES.append(i.text)\n",
    "    except NoSuchEementException:\n",
    "        NO_INSTANCES.append('-')\n",
    "        \n",
    "     #for scraping noof attributes:\n",
    "    try:\n",
    "        no_attribute=driver.find_elements(By.XPATH,'//div[@class=\"relative flex flex-col gap-4 bg-base-100 p-4 shadow\"][1]/div[2]/div[6]/p[1]')\n",
    "        for i in no_attribute:\n",
    "            NO_ATTRIBUTES.append(i.text)\n",
    "    except NoSuchEementException:   \n",
    "        NO_ATTRIBUTES.append('-')\n",
    "        \n",
    "print(len(DATASET_NAME),len(DATA_TYPE),len(TASK),len(ATTRIBUTE_TYPE),len(NO_INSTANCES),len(NO_ATTRIBUTES))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "083043c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATASET NAME</th>\n",
       "      <th>DATA_TYPE</th>\n",
       "      <th>TASK</th>\n",
       "      <th>ATTRIBUTE_TYPE</th>\n",
       "      <th>NO OF INSTANCES</th>\n",
       "      <th>NO OF ATTRIBUTES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Iris</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Real</td>\n",
       "      <td>150</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Heart Disease</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Categorical, Integer, Real</td>\n",
       "      <td>303</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adult</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Categorical, Integer</td>\n",
       "      <td>48842</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dry Bean Dataset</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Integer, Real</td>\n",
       "      <td>13611</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Diabetes</td>\n",
       "      <td>Multivariate, Time-Series</td>\n",
       "      <td>-</td>\n",
       "      <td>Categorical, Integer</td>\n",
       "      <td>-</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Wine</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Integer, Real</td>\n",
       "      <td>178</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Breast Cancer Wisconsin (Diagnostic)</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Real</td>\n",
       "      <td>569</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Rice (Cammeo and Osmancik)</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Real</td>\n",
       "      <td>3810</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Car Evaluation</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Categorical</td>\n",
       "      <td>1728</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Mushroom</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Categorical</td>\n",
       "      <td>8124</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           DATASET NAME                  DATA_TYPE  \\\n",
       "0                                  Iris               Multivariate   \n",
       "1                         Heart Disease               Multivariate   \n",
       "2                                 Adult               Multivariate   \n",
       "3                      Dry Bean Dataset               Multivariate   \n",
       "4                              Diabetes  Multivariate, Time-Series   \n",
       "5                                  Wine               Multivariate   \n",
       "6  Breast Cancer Wisconsin (Diagnostic)               Multivariate   \n",
       "7            Rice (Cammeo and Osmancik)               Multivariate   \n",
       "8                        Car Evaluation               Multivariate   \n",
       "9                              Mushroom               Multivariate   \n",
       "\n",
       "             TASK              ATTRIBUTE_TYPE NO OF INSTANCES NO OF ATTRIBUTES  \n",
       "0  Classification                        Real             150                4  \n",
       "1  Classification  Categorical, Integer, Real             303               13  \n",
       "2  Classification        Categorical, Integer           48842               14  \n",
       "3  Classification               Integer, Real           13611               16  \n",
       "4               -        Categorical, Integer               -               20  \n",
       "5  Classification               Integer, Real             178               13  \n",
       "6  Classification                        Real             569               30  \n",
       "7  Classification                        Real            3810                8  \n",
       "8  Classification                 Categorical            1728                6  \n",
       "9  Classification                 Categorical            8124               22  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data8={\"DATASET NAME\":DATASET_NAME,\"DATA_TYPE\":DATA_TYPE,\"TASK\":TASK,\"ATTRIBUTE_TYPE\":ATTRIBUTE_TYPE,\"NO OF INSTANCES\":NO_INSTANCES,\"NO OF ATTRIBUTES\":NO_ATTRIBUTES}\n",
    "df8=pd.DataFrame(data8)\n",
    "df8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414dae6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
